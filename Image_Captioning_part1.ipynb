{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Captioning_part1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTGN454d6Bx8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YOwYTAa6uJv"
      },
      "source": [
        "\n",
        "\n",
        "1.   Convert text to numerical value\n",
        "2.   We need a vocabulary mapping of each word to an index\n",
        "3.   setup a  pytorch dataset to load the data\n",
        "4.   Setup padding of every batch\n",
        "5.   Setup a dataloader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpcrerLq7eMj"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.optim as optim \n",
        "import torch.nn.functional as F # activation functions.....etc\n",
        "from torch.utils.data import DataLoader # easier dataset management\n",
        "import torchvision.datasets as datasets \n",
        "import torchvision.transforms as transforms \n",
        "import torchvision.models as models\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "import spacy # for tokenization \n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyfaYXhtCq-n"
      },
      "source": [
        "spacy_eng=spacy.load(\"en\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABzoDZDu-Ezk"
      },
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold):\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer_eng(text):\n",
        "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = {}\n",
        "        idx = 4\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer_eng(sentence):\n",
        "                if word not in frequencies:\n",
        "                    frequencies[word] = 1\n",
        "\n",
        "                else:\n",
        "                    frequencies[word] += 1\n",
        "\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        tokenized_text = self.tokenizer_eng(text)\n",
        "\n",
        "        return [\n",
        "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
        "            for token in tokenized_text\n",
        "        ]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVitShijCsO4"
      },
      "source": [
        "class MyCollate:\n",
        "    def __init__(self, pad_idx):\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "        imgs = torch.cat(imgs, dim=0)\n",
        "        targets = [item[1] for item in batch]\n",
        "        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n",
        "\n",
        "        return imgs, targets\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmTXaZjt7lkO"
      },
      "source": [
        "class FlickerDataset(Dataset):\n",
        "  def __init__(self,root,captions_file,transforms=None,freq_thershold=5):\n",
        "    self.root=root\n",
        "    self.df=pd.read_csv(captions_file)\n",
        "    self.transform=transform\n",
        "\n",
        "\n",
        "    # get image and caption col\n",
        "\n",
        "    self.img=self.df[\"image\"]\n",
        "    self.caption=self.df[\"caption\"]\n",
        "\n",
        "    #Initialize and build vocabulary\n",
        "\n",
        "    self.vocab=Vocabulary(freq_thershold)\n",
        "    self.vocab.build_vocabulary(self.caption.tolist())\n",
        "\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    caption=self.caption[index]\n",
        "    img_id=self.img[index]\n",
        "    img=Image.open(os.path.join(self.root,img_id)).convert(\"RGB\")\n",
        "\n",
        "    if(self.transform):\n",
        "      img=self.transform(img)\n",
        "    \n",
        "    numerical_caption=[self.vocab.stoi[\"<SOS>\"]]\n",
        "    numerical_caption+=self.vocab.numericalize(caption)\n",
        "    numerical_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
        "\n",
        "    return img,torch.tensor(numerical_caption)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA7wzBPDHaE7"
      },
      "source": [
        "def get_loader(root,annotation_file,transform,batch_size=32,num_workers=2,shuffle=True,pin_memory=True,):\n",
        "  dataset=FlickerDataset(root,annotation_file,transform)\n",
        "  pad_idx=dataset.vocab.stoi[\"<PAD>\"]\n",
        "  loader=DataLoader(\n",
        "      dataset=dataset,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=num_workers,\n",
        "      shuffle=shuffle,\n",
        "      pin_memory=pin_memory,\n",
        "      collate_fn=MyCollate(pad_idx),\n",
        "  )\n",
        "\n",
        "  return loader,dataset"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD-qQ9RbIZWq"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "                                        transforms.Resize((300,300)),\n",
        "                                        transforms.ToTensor()\n",
        "                                        ]\n",
        "                        \n",
        "                                       )\n",
        "dataloader,dataset=get_loader(\"/content/drive/MyDrive/Pytorch Tutorial/Datasets/flickr8k/images\",\n",
        "                      \"/content/drive/MyDrive/Pytorch Tutorial/Datasets/flickr8k/captions.txt\",\n",
        "                      transform = transform )"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGg1yC7zOWCf",
        "outputId": "98bc89f8-74ce-4e8e-a43c-7422d12c7e42"
      },
      "source": [
        "for idx,(img,caption) in enumerate(dataloader):\n",
        "  if(idx==1):\n",
        "    print(img.shape)\n",
        "    print(caption)\n",
        "    break"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 300, 300])\n",
            "tensor([[   1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1],\n",
            "        [   4,    4,  139,    4,    4,    4,   10,    4,    4,    4,   71,    4,\n",
            "           50,    4,    4,    4,    4,    4,    4,    4,   96,   71,    4,    4,\n",
            "            4,    4,   57,    4,    4,  574,  166,    4],\n",
            "        [  20,  402,   61,   14,   61,   14,  362,    6,   21,   80, 2564,    6,\n",
            "           51,   61,  196,   61,  610,   14,   14,   80,   98,   97,   80, 2214,\n",
            "          907,   56,   16,   20,   20, 2032,   97,   28],\n",
            "        [  16,  116,   68,   17,  145,   17,   17,  111,    6,    8,  122,   17,\n",
            "           34,   43,   80,   43,    8,   12,    8, 1462,  337,   34,   79,  316,\n",
            "            7,  111,  543,    6,    6,  629,  108, 1558],\n",
            "        [  21, 1021,  205,   32,    7,    3,  578,   13,   17,    4,    4,   29,\n",
            "           70,  111,   75,   41,    4,  328,    4,    8,    5,   46,    4,   99,\n",
            "           12,   12,  257,  111,   17,  404,   13,  102],\n",
            "        [   6,  117,   99,   67,   12,    4,    4,    4,  360,  151,  815,   90,\n",
            "           35,  188,    8,   12,   26,  421,    3,    4,    2,   13,   21,    4,\n",
            "            4,    4,  316,  188,  159,   26,   10,    4],\n",
            "        [  75,  316,  482,    4,    4,    3,  499,  230,   76, 1328,    8,    4,\n",
            "         1303,    4,   23,    4,   83,   16,  754,  128,    0,   10,   18,  320,\n",
            "         1653,   85,  155,   10,   12,    3,  112,  795],\n",
            "        [  13,   37,  771,  371,    3,   27,    8,  318,   10,  104,   10,   20,\n",
            "         1962,  722,   11,  217,    5, 1342,   17,  732,    0,  162,   16,    5,\n",
            "          137,  337, 1455,   52,    4,    2,    5,   12],\n",
            "        [  10,   10,  103,   53,    8,   71,   24,    5,  121,  511,  121,   16,\n",
            "            5,  339,    4,   36,    2,   92,   12, 1616,    0,   11,  206,    2,\n",
            "          173,    5,  193,    5,   36,    0,    2,   50],\n",
            "        [  22,  426, 2443,   33,  104,   61, 1560,    2,   27,   40,    5,   38,\n",
            "            2,    4,  446,    8,    0,    4,    4,   48,    0,    4,    4,    0,\n",
            "            8,    2,  922,    2,    8,    0,    0, 1867],\n",
            "        [  12,    5,    2,    5,  229, 2199, 2487,    0,   39,  104,    2,  701,\n",
            "            0,    3,   11,    4,    0, 1832,   80,  104,    0, 1055,  130,    0,\n",
            "            4,    0,   48,    0,   60,    0,    0,    8],\n",
            "        [   4,    2,    0,    2,    2, 2200,    5,    0,    4,  137,    0,    8,\n",
            "            0,    5,   44,   42,    0,  200,   79, 1285,    0,    5,   45,    0,\n",
            "           30,    0,   68,    0,   62,    0,    0,   64],\n",
            "        [ 348,    0,    0,    0,    0,    5,    2,    0,   36,    5,    0,   24,\n",
            "            0,    2,   12,    2,    0,    5,    3,  159,    0,    2,    4,    0,\n",
            "          632,    0,   69,    0,    5,    0,    0,    5],\n",
            "        [   8,    0,    0,    0,    0,    2,    0,    0,    8,    2,    0,  634,\n",
            "            0,    0,   44,    0,    0,    2,  663,  466,    0,    0, 1382,    0,\n",
            "           16,    0,    8,    0,    2,    0,    0,    2],\n",
            "        [  60,    0,    0,    0,    0,    0,    0,    0,  136,    0,    0,  440,\n",
            "            0,    0, 1105,    0,    0,    0,    5,    5,    0,    0,  481,    0,\n",
            "           57,    0,   22,    0,    0,    0,    0,    0],\n",
            "        [  62,    0,    0,    0,    0,    0,    0,    0,   62,    0,    0,    5,\n",
            "            0,    0,    5,    0,    0,    0,    2,    2,    0,    0,   45,    0,\n",
            "          955,    0,    2,    0,    0,    0,    0,    0],\n",
            "        [   2,    0,    0,    0,    0,    0,    0,    0,    5,    0,    0,    2,\n",
            "            0,    0,    2,    0,    0,    0,    0,    0,    0,    0,   16,    0,\n",
            "          141,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [   0,    0,    0,    0,    0,    0,    0,    0,    2,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    4,    0,\n",
            "            5,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   57,    0,\n",
            "            2,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    3,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  481,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   45,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    3,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  104,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1196,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    5,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    2,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8biXPT2HShSO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}