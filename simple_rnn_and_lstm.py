# -*- coding: utf-8 -*-
"""Simple RNN and LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MBLfgzM7Eo-hs6fEWfafSz5jYkh4AXNq
"""

#importing


import torch
import torch.nn as nn 
import torch.optim as optim 
import torch.nn.functional as F # activation functions.....etc
from torch.utils.data import DataLoader # easier dataset management
import torchvision.datasets as datasets 
import torchvision.transforms as transforms

#set device

device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')

"""Image shape : N X 1 X 28 X 28 

RNN: 28 timesteps, 28 features for each timestep
"""

input_size=28  # features
seq_len=28 # timesteps
num_layers=2
hidden_size=256
lr=0.001
num_classes=10
batch_size=64
num_epochs=5
load_model=True

class RNN(nn.Module):
  def __init__(self,input_size,hidden_size,num_layers,num_classes):
    super(RNN,self).__init__()

    self.hidden_size=hidden_size
    self.num_layers=num_layers
    self.rnn=nn.RNN(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers,batch_first=True) # return seq is true here
    #Batch_size X time seq X num features

    self.fc=nn.Linear(hidden_size*seq_len,num_classes)
  
  def forward(self,x):
    h0=torch.zeros(self.num_layers,x.size(0),self.hidden_size).to(device)

    #forward prop
    out,_=self.rnn(x,h0)
    out=out.reshape(out.shape[0],-1)
    out=self.fc(out)

    return out

class LSTM(nn.Module):
  def __init__(self,input_size,hidden_size,num_layers,num_classes):
    super(LSTM,self).__init__()

    self.hidden_size=hidden_size
    self.num_layers=num_layers
    self.rnn=nn.LSTM(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers,batch_first=True) # return seq is true here
    #Batch_size X time seq X num features

    self.fc=nn.Linear(hidden_size*seq_len,num_classes)
  
  def forward(self,x):
    h0=torch.zeros(self.num_layers,x.size(0),self.hidden_size).to(device)
    c0=torch.zeros(self.num_layers,x.size(0),self.hidden_size).to(device)

    #forward prop
    out,_=self.rnn(x,(h0,c0))
    out=out.reshape(out.shape[0],-1)
    out=self.fc(out)

    return out

  

def save_checkpoint(state,filename="my_checkpoint.pth.tor"):
  print("Saving...")
  torch.save(state,filename) 


def load_checkpoint(checkpoint):
  print("loading...")
  model.load_state_dict(checkpoint['state_dict'])
  opt.load_state_dict(checkpoint['optimizer'])

#load data

train_dataset=datasets.MNIST(root='dataset/',train=True,transform=transforms.ToTensor(),download=True)

train_loader=DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)  # loads the data from folder in batchwise manner

test_dataset=datasets.MNIST(root='dataset/',train=False,transform=transforms.ToTensor(),download=True)

test_loader=DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=True)  # loads the data from folder in batchwise manner

#initialize network

model= RNN(input_size,hidden_size,num_layers,num_classes).to(device)


#loss and optimizer

loss_function=nn.CrossEntropyLoss()
opt=optim.Adam(model.parameters(),lr=lr)



#train

for epoch in range(num_epochs):
  for batch_idx,(data,targets) in enumerate(train_loader): # enumerate is used just to keep track which batch index is now being processed, and  train loader has two parts, data and target
    #get data to Cuda if possible
    data=data.to(device=device).squeeze(1)
    targets=targets.to(device=device)

    if(batch_idx==1):
      print(data.shape)  # just to check if the size is OK

    #reshape data
    #data=data.reshape(data.shape[0],-1)

    #forward pass
    scores=model(data) # get the result from forward pass
    loss=loss_function(scores,targets) # calculate loss


    #backward
    opt.zero_grad() #In PyTorch, for every mini-batch during the training phase, we need to explicitly set the gradients to zero
                    #before starting to do backpropragation (i.e., updation of Weights and biases) because PyTorch accumulates the gradients on 
                    #subsequent backward passes. This is convenient while training RNNs. So, the default action has been set to accumulate (i.e. sum)
                    # the gradients on every loss.backward() call
    loss.backward() 
    opt.step()



#check accuracy 

def accuracy_check(loader,model): # get the dataset(loader here), and trained model

  if(loader.dataset.train):
    print("Train accuracy: ")
  else:
    print("Test accuracy: ")

  num_correct=0
  num_samples=0

  model.eval()

  with torch.no_grad(): # no need to train now
    for x,y in loader:
      x=x.to(device=device).squeeze(1)
      y=y.to(device=device)

      #x=x.reshape(x.shape[0],-1)
      scores=model(x)
      #64 X 10

      _,predictions=scores.max(1) # get the max index of each ROW
      num_correct+=(predictions==y).sum() # here in each iteration of the loop, a batch of size 64 will be passed here. In each batch ,
                                          #we have 64 data. We sum all these 64 data predictions ( 1 or 0)
      num_samples+=predictions.size(0)

    acc=num_correct/num_samples
    print(acc)



accuracy_check(train_loader,model)
accuracy_check(test_loader,model)



#initialize network

model= LSTM(input_size,hidden_size,num_layers,num_classes).to(device)


#loss and optimizer

loss_function=nn.CrossEntropyLoss()
opt=optim.Adam(model.parameters(),lr=lr)






#train
if(load_model):
  load_checkpoint(torch.load("/content/my_checkpoint.pth.tor"))

for epoch in range(num_epochs):
  losses=[]

  if(epoch%3==0):
    checkpoint={'state_dict': model.state_dict(),'optimizer':opt.state_dict()}
    save_checkpoint(checkpoint)
  
  for batch_idx,(data,targets) in enumerate(train_loader): # enumerate is used just to keep track which batch index is now being processed, and  train loader has two parts, data and target
    #get data to Cuda if possible
    data=data.to(device=device).squeeze(1)
    targets=targets.to(device=device)

    if(batch_idx==1):
      print(data.shape)  # just to check if the size is OK

    #reshape data
    #data=data.reshape(data.shape[0],-1)

    #forward pass
    scores=model(data) # get the result from forward pass
    loss=loss_function(scores,targets) # calculate loss
    losses.append(loss.item())
    


    #backward
    opt.zero_grad() #In PyTorch, for every mini-batch during the training phase, we need to explicitly set the gradients to zero
                    #before starting to do backpropragation (i.e., updation of Weights and biases) because PyTorch accumulates the gradients on 
                    #subsequent backward passes. This is convenient while training RNNs. So, the default action has been set to accumulate (i.e. sum)
                    # the gradients on every loss.backward() call
    loss.backward() 
    opt.step()
  mean_loss=sum(losses)/len(losses)
  print("Loss at epoch: "+str(epoch)+ " is: "+str(mean_loss))


#check accuracy 

def accuracy_check(loader,model): # get the dataset(loader here), and trained model

  if(loader.dataset.train):
    print("Train accuracy: ")
  else:
    print("Test accuracy: ")

  num_correct=0
  num_samples=0

  model.eval()

  with torch.no_grad(): # no need to train now
    for x,y in loader:
      x=x.to(device=device).squeeze(1)
      y=y.to(device=device)

      #x=x.reshape(x.shape[0],-1)
      scores=model(x)
      #64 X 10

      _,predictions=scores.max(1) # get the max index of each ROW
      num_correct+=(predictions==y).sum() # here in each iteration of the loop, a batch of size 64 will be passed here. In each batch ,
                                          #we have 64 data. We sum all these 64 data predictions ( 1 or 0)
      num_samples+=predictions.size(0)

    acc=num_correct/num_samples
    print(acc)

accuracy_check(train_loader,model)
accuracy_check(test_loader,model)